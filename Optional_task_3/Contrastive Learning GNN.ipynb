{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset as TorchDataset\n",
    "import h5py\n",
    "import torch\n",
    "from torch_geometric.data import Dataset as PygDataset, Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MLP, DynamicEdgeConv, global_max_pool, Linear\n",
    "import torch_geometric.transforms as T\n",
    "from tqdm import tqdm\n",
    "from pytorch_metric_learning.losses import NTXentLoss\n",
    "cpu_count = mp.cpu_count()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "quark_gluon_path = \"../Data/hdf5/processed/quark-gluon-dataset.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_dataset(raw_path, processed_path, subset_len = 6000, starter = 0):\n",
    "    with h5py.File(raw_path, 'r') as f, h5py.File(processed_path, 'w') as p:\n",
    "        keys = list(f.keys())\n",
    "        total_events = f[keys[1]].shape[0]\n",
    "        for key in keys:\n",
    "            shape = (subset_len,)\n",
    "            if len(f[key].shape) > 1:\n",
    "                shape = (subset_len, 125, 125, 3)\n",
    "            p.create_dataset(key, shape=shape)\n",
    "        quark_count = 0\n",
    "        gluon_count = 0\n",
    "        idx = 0\n",
    "        for i in range(starter, starter + subset_len):\n",
    "            if quark_count < subset_len // 2:\n",
    "                for key in keys:\n",
    "                    p[key][idx] = f[key][i]\n",
    "                quark_count += 1\n",
    "                idx += 1\n",
    "            elif gluon_count < subset_len // 2:\n",
    "                for key in keys:\n",
    "                    p[key][idx] = f[key][i]\n",
    "                gluon_count += 1\n",
    "                idx+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"../Data/hdf5/processed/train.hdf5\"\n",
    "val_path = \"../Data/hdf5/processed/val.hdf5\"\n",
    "test_path = \"../Data/hdf5/processed/test.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_dataset(quark_gluon_path, train_path, 6000)\n",
    "subset_dataset(quark_gluon_path, val_path, 1200, 6000)\n",
    "subset_dataset(quark_gluon_path, test_path, 1200, 7200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pillow(x):\n",
    "    return x.transpose((2,1,0))\n",
    "def get_k_nearest(indices, k = 10):\n",
    "    edges = None\n",
    "    for i in range(indices.shape[0]):\n",
    "        k_nearest = np.sum((indices - indices[i])**2, axis=1).argsort()\n",
    "        k_nearest_edges = np.array([[i, j] for j in k_nearest[1:k]])\n",
    "        if edges is None:\n",
    "            edges = k_nearest_edges\n",
    "        else:\n",
    "            edges = np.vstack((edges, k_nearest_edges))\n",
    "    return edges\n",
    "def create_graph(idx,quark_gluon_path ,outpath ):\n",
    "    data = Data()\n",
    "    with h5py.File(quark_gluon_path, 'r') as f:\n",
    "        y = f['y'][idx]\n",
    "        x = f['X_jets'][idx]\n",
    "        non_zero_indices = np.argwhere(np.sum(x, axis=2))\n",
    "        non_zero_fetures = x[non_zero_indices[:, 0], non_zero_indices[:, 1]]\n",
    "        data.x = torch.from_numpy(non_zero_fetures)\n",
    "        edges = get_k_nearest(non_zero_indices)\n",
    "        data.edge_index = torch.from_numpy(edges).t().contiguous().to(torch.float64)\n",
    "        data.y = torch.from_numpy(np.asarray([y]))\n",
    "        data.pos = torch.from_numpy(non_zero_indices).to(torch.float64)\n",
    "        torch.save(data, osp.join(outpath, f\"{idx}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grapher(root_dir = \"../Data/hdf5/processed\"):\n",
    "    files = [\"train.hdf5\", \"val.hdf5\", \"test.hdf5\"]\n",
    "    for file in files:\n",
    "        path = osp.join(root_dir , file)\n",
    "        with h5py.File(path, 'r') as f:\n",
    "            event_count = len(f[\"X_jets\"])\n",
    "        data = file.split(\".\")[0]\n",
    "        if len(os.listdir(\"../Data/Graphs/{}/raw\".format(data))) < 1:\n",
    "            for i in range(event_count):\n",
    "                # print(data)\n",
    "                create_graph(i, path , \"../Data/Graphs/{}/raw\".format(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "grapher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuarkGluonGraphs(PygDataset):\n",
    "    def __init__(self, root = None, transform = None, pre_transform = None, pre_filter = None, log = True):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter, log)\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return os.listdir(osp.join(self.root, \"raw\"))\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return os.listdir(osp.join(self.root, \"raw\"))\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        for raw_path in self.raw_file_names:\n",
    "            data = torch.load(osp.join(self.raw_dir, raw_path))\n",
    "            data.y = F.one_hot(data.y.to(torch.int64), 2)\n",
    "            torch.save(data, osp.join(self.processed_dir, raw_path))\n",
    "    def len(self):\n",
    "        return len(self.processed_file_names)\n",
    "    def get(self, idx):\n",
    "        data = torch.load(osp.join(self.processed_dir, f\"{idx}.pt\"))\n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n",
      "Processing...\n",
      "Done!\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = \"cpu\"\n",
    "augmentation = T.Compose([\n",
    "    T.NormalizeFeatures(),\n",
    "    T.RandomJitter(0.03),\n",
    "    T.RandomFlip(1),\n",
    "    T.RandomShear(0.2),\n",
    "    T.ToDevice(device),\n",
    "])\n",
    "train_data = QuarkGluonGraphs(\"../Data/Graphs/train/\")\n",
    "val_data = QuarkGluonGraphs(\"../Data/Graphs/val/\")\n",
    "test_data = QuarkGluonGraphs(\"../Data/Graphs/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, k = 2, aggr = \"max\"):\n",
    "        super().__init__()\n",
    "        num_classes = 2\n",
    "        embedding_size = 1024\n",
    "\n",
    "        # Feature extraction\n",
    "        self.conv1 = DynamicEdgeConv(MLP([2*3, 64, 64]), k, aggr)\n",
    "        self.conv2 = DynamicEdgeConv(MLP([2*64, 128]), k, aggr)\n",
    "\n",
    "        # Encoder head\n",
    "        self.lin1 = Linear(128+64, 128)\n",
    "\n",
    "        # Projection head \n",
    "        self.mlp = MLP([128, 256,32], norm=None)\n",
    "\n",
    "    def forward(self, data, train = True):\n",
    "        if train:\n",
    "            # get 2 augmentations of the batch\n",
    "            augm_1 = augmentation(data)\n",
    "            augm_2 = augmentation(data)\n",
    "            \n",
    "            # extract properties\n",
    "            pos_1, batch_1 = augm_1.pos, augm_1.batch\n",
    "            pos_2, batch_2 = augm_2.pos, augm_2.batch\n",
    "\n",
    "            # Get representation for the first augmented view\n",
    "            x1 = self.conv1(pos_1, batch_1)\n",
    "            x2 = self.conv2(x1, batch_1)\n",
    "            \n",
    "            h_points_1 = self.lin1(torch.cat([x1, x2], dim=1))\n",
    "\n",
    "            # Get representation for the second augmented view\n",
    "            x1 = self.conv1(pos_2, batch_2)\n",
    "            x2 = self.conv2(x1, batch_2)\n",
    "            h_points_2 = self.lin1(torch.cat([x1, x2], dim=1))\n",
    "            \n",
    "\n",
    "            # Global representation\n",
    "            h_1 = global_max_pool(h_points_1, batch_1)\n",
    "            h_2 = global_max_pool(h_points_2, batch_2)\n",
    "\n",
    "        else:\n",
    "            x1 = self.conv1(data.pos, data.batch)\n",
    "            x2 = self.conv2(x1, data.batch)\n",
    "            h_points = self.lin1(torch.cat([x1, x2], dim=1))\n",
    "            return global_max_pool(h_points)\n",
    "        \n",
    "\n",
    "        # Transormation for loss function\n",
    "        compact_h_1 = self.mlp(h_1)\n",
    "        compact_h_2 = self.mlp(h_2)\n",
    "        return (h_1, h_2, compact_h_1, compact_h_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = NTXentLoss(temperature=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "data_loader = DataLoader(train_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for _, data in enumerate(tqdm.tqdm(data_loader)):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # Get data representations\n",
    "        h_1, h_2, compact_h_1, compact_h_2 = model(data)\n",
    "        \n",
    "        # Prepare for loss\n",
    "        embeddings = torch.cat((compact_h_1, compact_h_2))\n",
    "        # The same index corresponds to a positive pair\n",
    "        indices = torch.arange(0, compact_h_1.size(0), device=compact_h_2.device)\n",
    "        labels = torch.cat((indices, indices))\n",
    "        loss = loss_func(embeddings, labels)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/188 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m4\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     loss \u001b[39m=\u001b[39m train()\n\u001b[1;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m:\u001b[39;00m\u001b[39m03d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m     scheduler\u001b[39m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[21], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m      9\u001b[0m \u001b[39m# Get data representations\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m h_1, h_2, compact_h_1, compact_h_2 \u001b[39m=\u001b[39m model(data)\n\u001b[1;32m     12\u001b[0m \u001b[39m# Prepare for loss\u001b[39;00m\n\u001b[1;32m     13\u001b[0m embeddings \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((compact_h_1, compact_h_2))\n",
      "File \u001b[0;32m~/miniconda3/envs/gnn/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[18], line 28\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, data, train)\u001b[0m\n\u001b[1;32m     25\u001b[0m pos_2, batch_2 \u001b[39m=\u001b[39m augm_2\u001b[39m.\u001b[39mpos, augm_2\u001b[39m.\u001b[39mbatch\n\u001b[1;32m     27\u001b[0m \u001b[39m# Get representation for the first augmented view\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m x1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(pos_1, batch_1)\n\u001b[1;32m     29\u001b[0m x2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x1, batch_1)\n\u001b[1;32m     30\u001b[0m \u001b[39mprint\u001b[39m(x1\u001b[39m.\u001b[39mtype, x2\u001b[39m.\u001b[39mtype)\n",
      "File \u001b[0;32m~/miniconda3/envs/gnn/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/gnn/lib/python3.9/site-packages/torch_geometric/nn/conv/edge_conv.py:138\u001b[0m, in \u001b[0;36mDynamicEdgeConv.forward\u001b[0;34m(self, x, batch)\u001b[0m\n\u001b[1;32m    135\u001b[0m edge_index \u001b[39m=\u001b[39m knn(x[\u001b[39m0\u001b[39m], x[\u001b[39m1\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk, b[\u001b[39m0\u001b[39m], b[\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mflip([\u001b[39m0\u001b[39m])\n\u001b[1;32m    137\u001b[0m \u001b[39m# propagate_type: (x: PairTensor)\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate(edge_index, x\u001b[39m=\u001b[39;49mx, size\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/gnn/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:437\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m         msg_kwargs \u001b[39m=\u001b[39m res[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(res, \u001b[39mtuple\u001b[39m) \u001b[39melse\u001b[39;00m res\n\u001b[0;32m--> 437\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmessage(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmsg_kwargs)\n\u001b[1;32m    438\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_message_forward_hooks\u001b[39m.\u001b[39mvalues():\n\u001b[1;32m    439\u001b[0m     res \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, (msg_kwargs, ), out)\n",
      "File \u001b[0;32m~/miniconda3/envs/gnn/lib/python3.9/site-packages/torch_geometric/nn/conv/edge_conv.py:141\u001b[0m, in \u001b[0;36mDynamicEdgeConv.message\u001b[0;34m(self, x_i, x_j)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmessage\u001b[39m(\u001b[39mself\u001b[39m, x_i: Tensor, x_j: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 141\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnn(torch\u001b[39m.\u001b[39;49mcat([x_i, x_j \u001b[39m-\u001b[39;49m x_i], dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n",
      "File \u001b[0;32m~/miniconda3/envs/gnn/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/gnn/lib/python3.9/site-packages/torch_geometric/nn/models/mlp.py:183\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x, return_emb)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[39mfor\u001b[39;00m i, (lin, norm) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlins, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorms)):\n\u001b[0;32m--> 183\u001b[0m     x \u001b[39m=\u001b[39m lin(x)\n\u001b[1;32m    184\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact_first:\n\u001b[1;32m    185\u001b[0m         x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/gnn/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/gnn/lib/python3.9/site-packages/torch_geometric/nn/dense/linear.py:136\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    132\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[39m        x (Tensor): The features.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(1, 4):\n",
    "    loss = train()\n",
    "    print(f'Epoch {epoch:03d}, Loss: {loss:.4f}')\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_data:\n",
    "    data = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[884, 3], edge_index=[2, 7956], y=[1, 2], pos=[884, 2])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a7e9d0b6ec238a7d2d97d894925170bb7e5915a26ee8ae88ee5a8209d1d0e607"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
