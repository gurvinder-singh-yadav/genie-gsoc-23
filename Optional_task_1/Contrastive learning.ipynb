{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific Task 1 <br>\n",
    "## (if you are interested in “Deep Graph Anomaly Detection with Contrastive Learning” project):\n",
    "\n",
    "<ol>\n",
    "<li> Classify the quark/gluon data with a model that learns data representation with a contrastive loss.\n",
    "<li> Evaluate the classification performance on a test dataset.\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch import nn, optim\n",
    "import torch.functional as F\n",
    "import torchvision\n",
    "import h5py\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import torchmetrics as tm\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "%load_ext tensorboard\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_dataset(raw_path, processed_path, subset_len = 6000):\n",
    "    with h5py.File(raw_path, 'r') as f, h5py.File(processed_path, 'w') as p:\n",
    "        keys = list(f.keys())\n",
    "        total_events = f[keys[1]].shape[0]\n",
    "        for key in keys:\n",
    "            shape = (subset_len,)\n",
    "            if len(f[key].shape) > 1:\n",
    "                shape = (subset_len, 125, 125, 3)\n",
    "            p.create_dataset(key, shape=shape)\n",
    "        quark_count = 0\n",
    "        gluon_count = 0\n",
    "        idx = 0\n",
    "        for i in range(total_events):\n",
    "            if quark_count < subset_len // 2:\n",
    "                for key in keys:\n",
    "                    p[key][idx] = f[key][idx]\n",
    "                quark_count += 1\n",
    "                idx += 1\n",
    "            elif gluon_count < subset_len // 2:\n",
    "                for key in keys:\n",
    "                    p[key][idx] = f[key][idx]\n",
    "                gluon_count += 1\n",
    "                idx += 1\n",
    "            elif idx >= subset_len:\n",
    "                break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncompressed_data_path = \"../Data/hdf5/processed/quark-gluon-dataset.hdf5\"\n",
    "subset_data_path = \"../Data/hdf5/processed/processed.hdf5\"\n",
    "CHECKPOINT_PATH = \"saved_models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(subset_data_path):\n",
    "    subset_dataset(uncompressed_data_path, subset_data_path, subset_len=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuarkGluonDataset(Dataset):\n",
    "    def __init__(self, path, transform = None) -> None:\n",
    "        super().__init__()\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        with h5py.File(self.path, 'r') as f:\n",
    "            self.keys = list(f.keys())\n",
    "    def __len__(self):\n",
    "        with h5py.File(self.path, 'r') as f:\n",
    "            return len(f[self.keys[1]])\n",
    "    def __getitem__(self, index):\n",
    "        with h5py.File(self.path, 'r') as f:\n",
    "            x = f[self.keys[0]][index]\n",
    "            y = np.array(f['y'][index])\n",
    "            y = torch.from_numpy(y)\n",
    "            # y = torch.nn.functional.one_hot(y.long(), 2)\n",
    "            x = torch.from_numpy(x)\n",
    "            x = torch.permute(x, (2, 0, 1)) # convert (n, n, 3) -> (3, n, n)\n",
    "            if self.transform is not None:\n",
    "                x = self.transform(x)\n",
    "                return x, y\n",
    "            return x, y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveTransformations(object):\n",
    "\n",
    "    def __init__(self, base_transforms, n_views=2):\n",
    "        self.base_transforms = base_transforms\n",
    "        self.n_views = n_views\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return [self.base_transforms(x) for i in range(self.n_views)]\n",
    "    \n",
    "contrast_transforms = transforms.Compose([\n",
    "                                          transforms.ToPILImage(),\n",
    "                                          transforms.RandomResizedCrop(size=96),\n",
    "                                          transforms.RandomApply([\n",
    "                                              transforms.ColorJitter(brightness=0.5,\n",
    "                                                                     contrast=0.5,\n",
    "                                                                     saturation=0.5,\n",
    "                                                                     hue=0.1)\n",
    "                                          ], p=0.8),\n",
    "                                          transforms.ToTensor(),\n",
    "                                          transforms.Normalize((0.5,), (0.5,)),\n",
    "                                         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(dataset, train = 0.6, val = 0.2, test = 0.2):\n",
    "    train_data, val_data, test_data = random_split(dataset, [train, val, test])\n",
    "    datasets = {}\n",
    "    datasets['train'] = train_data\n",
    "    datasets['val'] = val_data\n",
    "    datasets['test'] = test_data\n",
    "    return datasets\n",
    "\n",
    "cpu_count = mp.cpu_count()\n",
    "\n",
    "class QuarkGluonDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,dataset, batch_size = 64) -> None:\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.dataset = dataset\n",
    "    def setup(self, stage:str):\n",
    "        self.train_data = self.dataset['train']\n",
    "        self.val_data = self.dataset['val']\n",
    "        self.test_data = self.dataset['test']\n",
    "    def get_train(self, idx):\n",
    "        return self.train_data[idx][0]\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, batch_size=self.batch_size, shuffle=True, \n",
    "                          num_workers=cpu_count, prefetch_factor=2* cpu_count)\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_data, batch_size=self.batch_size, shuffle=False,\n",
    "                           num_workers=cpu_count, prefetch_factor=2* cpu_count)\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_data, batch_size=self.batch_size, shuffle=False, \n",
    "                          num_workers=cpu_count, prefetch_factor=2* cpu_count)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, hidden_dim, lr, temperature, weight_decay, max_epochs=500):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        assert self.hparams.temperature > 0.0, 'The temperature must be a positive float!'\n",
    "        # Base model f(.)\n",
    "        self.convnet = torchvision.models.resnet18(num_classes=4*hidden_dim)  # Output of last linear layer\n",
    "        # The MLP for g(.) consists of Linear->ReLU->Linear\n",
    "        self.convnet.fc = nn.Sequential(\n",
    "            self.convnet.fc,  # Linear(ResNet output, 4*hidden_dim)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4*hidden_dim, hidden_dim),\n",
    "            nn.Softmax()\n",
    "\n",
    "        )\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(),\n",
    "                                lr=self.hparams.lr,\n",
    "                                weight_decay=self.hparams.weight_decay)\n",
    "        lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                                                            T_max=self.hparams.max_epochs,\n",
    "                                                            eta_min=self.hparams.lr/50)\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def info_nce_loss(self, batch, mode='train'):\n",
    "        # print(batch.shape)\n",
    "        imgs, _ = batch\n",
    "        \n",
    "        # Encode all images\n",
    "        feats = self.convnet(imgs) \n",
    "        # Calculate cosine similarity\n",
    "        cos_sim = torch.nn.functional.cosine_similarity(feats[:,None,:], feats[None,:,:], dim=-1)\n",
    "        # Mask out cosine similarity to itself\n",
    "        self_mask = torch.eye(cos_sim.shape[0], dtype=torch.bool, device=cos_sim.device)\n",
    "        cos_sim.masked_fill_(self_mask, -9e15)\n",
    "        # Find positive example -> batch_size//2 away from the original example\n",
    "        pos_mask = self_mask.roll(shifts=cos_sim.shape[0]//2, dims=0)\n",
    "        # InfoNCE loss\n",
    "        cos_sim = cos_sim / self.hparams.temperature\n",
    "        nll = -cos_sim[pos_mask] + torch.logsumexp(cos_sim, dim=-1)\n",
    "        nll = nll.mean()\n",
    "\n",
    "        # Logging loss\n",
    "        self.log(mode+'_loss', nll)\n",
    "        # Get ranking position of positive example\n",
    "        comb_sim = torch.cat([cos_sim[pos_mask][:,None],  # First position positive example\n",
    "                              cos_sim.masked_fill(pos_mask, -9e15)],\n",
    "                             dim=-1)\n",
    "        sim_argsort = comb_sim.argsort(dim=-1, descending=True).argmin(dim=-1)\n",
    "        # Logging ranking metrics\n",
    "        self.log(mode+'_acc_top1', (sim_argsort == 0).float().mean())\n",
    "        self.log(mode+'_acc_top5', (sim_argsort < 5).float().mean())\n",
    "        self.log(mode+'_acc_mean_pos', 1+sim_argsort.float().mean())\n",
    "\n",
    "        return nll\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.info_nce_loss(batch, mode='train')\n",
    "    \n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.info_nce_loss(batch, mode='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(batch_size, max_epochs=500, **kwargs):\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, 'CLossCNN'),\n",
    "                         accelerator=\"gpu\",\n",
    "                         devices=1,\n",
    "                         max_epochs=max_epochs,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode='max', monitor='val_acc_top5'),\n",
    "                                    LearningRateMonitor('epoch')],\n",
    "                        enable_progress_bar=False)\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, 'CLossCNN.ckpt')\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(f'Found pretrained model at {pretrained_filename}, loading...')\n",
    "        model = Model.load_from_checkpoint(pretrained_filename) # Automatically loads the model \n",
    "        # with the saved hyperparameters\n",
    "    else:\n",
    "        pl.seed_everything(42) # To be reproducable\n",
    "        dataset = QuarkGluonDataset(subset_data_path, transform=contrast_transforms)\n",
    "        dataset = train_val_test_split(dataset)\n",
    "        dataset = QuarkGluonDataModule(dataset, batch_size=batch_size)\n",
    "        model = Model(max_epochs=max_epochs, **kwargs)\n",
    "        trainer.fit(model, datamodule=dataset)\n",
    "        # Load best checkpoint after training\n",
    "        model = model.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type   | Params\n",
      "-----------------------------------\n",
      "0 | convnet | ResNet | 11.2 M\n",
      "-----------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.723    Total estimated model params size (MB)\n",
      "/home/guru/miniconda3/envs/gnn/lib/python3.9/site-packages/torch/nn/modules/container.py:204: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n",
      "`Trainer.fit` stopped: `max_epochs=500` reached.\n"
     ]
    }
   ],
   "source": [
    "CLossCNN = train_model(batch_size=64,\n",
    "                         hidden_dim = 2,\n",
    "                         lr=5e-4,\n",
    "                         temperature=0.07,\n",
    "                         weight_decay=1e-4,\n",
    "                         max_epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = QuarkGluonDataset(subset_data_path, transform=contrast_transforms)\n",
    "dataset = train_val_test_split(dataset)\n",
    "test_loader = DataLoader(dataset['test'], batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    for batch in test_loader:\n",
    "        pred = CLossCNN.convnet(batch[0])\n",
    "        all_preds.append(pred)\n",
    "        all_labels.append(batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = torch.cat(all_labels)\n",
    "all_preds = torch.cat(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir ./saved_models/CLossCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tm.Accuracy(task=\"binary\", num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5192)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(all_preds.argmax(dim=1), all_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:  <br>\n",
    "[model Architecture](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial17/SimCLR.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a7e9d0b6ec238a7d2d97d894925170bb7e5915a26ee8ae88ee5a8209d1d0e607"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
